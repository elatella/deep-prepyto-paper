<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Amin Khosrozadeh" />
  <meta name="author" content="Raphaela Seeger" />
  <meta name="author" content="Julika Radecke" />
  <meta name="author" content="Guillaume Witz" />
  <meta name="author" content="Jakob B. Sørensen" />
  <meta name="author" content="Benoît Zuber" />
  <meta name="dcterms.date" content="2022-07-15" />
  <meta name="keywords" content="synapse, cryo-electron tomography, synaptic vesicles, deep learning, segmentation, post-processing, automation" />
  <title>Deep-learning based automatic segmentation of vesicles in cryo-electron tomograms</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta name="dc.title" content="Deep-learning based automatic segmentation of vesicles in cryo-electron tomograms" />
  <meta name="citation_title" content="Deep-learning based automatic segmentation of vesicles in cryo-electron tomograms" />
  <meta property="og:title" content="Deep-learning based automatic segmentation of vesicles in cryo-electron tomograms" />
  <meta property="twitter:title" content="Deep-learning based automatic segmentation of vesicles in cryo-electron tomograms" />
  <meta name="dc.date" content="2022-07-15" />
  <meta name="citation_publication_date" content="2022-07-15" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Amin Khosrozadeh" />
  <meta name="citation_author_institution" content="Institute of Anatomy, University of Bern, Bern, Switzerland" />
  <meta name="citation_author_institution" content="Graduate School for Cellular and Biomedical Sciences, University of Bern" />
  <meta name="citation_author_orcid" content="XXXX-XXXX-XXXX-XXXX" />
  <meta name="citation_author" content="Raphaela Seeger" />
  <meta name="citation_author_institution" content="Institute of Anatomy, University of Bern, Bern, Switzerland" />
  <meta name="citation_author_institution" content="Graduate School for Cellular and Biomedical Sciences, University of Bern" />
  <meta name="citation_author_orcid" content="XXXX-XXXX-XXXX-XXXX" />
  <meta name="citation_author" content="Julika Radecke" />
  <meta name="citation_author_institution" content="Institute of Anatomy, University of Bern, Bern, Switzerland" />
  <meta name="citation_author_institution" content="Department of Neuroscience, Faculty of Health and Medical Science , 2200 Copenhagen N, University of Copenhagen, Copenhagen, Denmark" />
  <meta name="citation_author_institution" content="Diamond Light Source Ltd, Didcot, Oxfordshire, United Kingdom" />
  <meta name="citation_author_orcid" content="0000-0002-5815-5537" />
  <meta name="citation_author" content="Guillaume Witz" />
  <meta name="citation_author_institution" content="Science IT Service, University of Bern, Bern, Switzerland" />
  <meta name="citation_author_institution" content="Microscopy Imaging Center, University of Bern, Bern, Switzerland" />
  <meta name="citation_author_orcid" content="0000-0003-1562-4265" />
  <meta name="citation_author" content="Jakob B. Sørensen" />
  <meta name="citation_author_institution" content="Department of Neuroscience, University of Copenhagen, Blegdamsvej 3B, 2200 Copenhagen N, Denmark" />
  <meta name="citation_author_orcid" content="0000-0001-5465-3769" />
  <meta name="citation_author" content="Benoît Zuber" />
  <meta name="citation_author_institution" content="Institute of Anatomy, University of Bern, Bern, Switzerland" />
  <meta name="citation_author_orcid" content="0000-0001-7725-5579" />
  <link rel="canonical" href="https://elatella.github.io/deep-prepyto-paper/" />
  <meta property="og:url" content="https://elatella.github.io/deep-prepyto-paper/" />
  <meta property="twitter:url" content="https://elatella.github.io/deep-prepyto-paper/" />
  <meta name="citation_fulltext_html_url" content="https://elatella.github.io/deep-prepyto-paper/" />
  <meta name="citation_pdf_url" content="https://elatella.github.io/deep-prepyto-paper/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://elatella.github.io/deep-prepyto-paper/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://elatella.github.io/deep-prepyto-paper/v/202cfa31a31390c7e2e4b838b6778fb133f1b295/" />
  <meta name="manubot_html_url_versioned" content="https://elatella.github.io/deep-prepyto-paper/v/202cfa31a31390c7e2e4b838b6778fb133f1b295/" />
  <meta name="manubot_pdf_url_versioned" content="https://elatella.github.io/deep-prepyto-paper/v/202cfa31a31390c7e2e4b838b6778fb133f1b295/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  
  <!-- pandoc-eqnos: equation style -->
  <style>
    .eqnos { display: inline-block; position: relative; width: 100%; }
    .eqnos br { display: none; }
    .eqnos-number { position: absolute; right: 0em; top: 50%; line-height: 0; }
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">Deep-learning based automatic segmentation of vesicles in cryo-electron tomograms</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://elatella.github.io/deep-prepyto-paper/v/202cfa31a31390c7e2e4b838b6778fb133f1b295/">permalink</a>)
was automatically generated
from <a href="https://github.com/elatella/deep-prepyto-paper/tree/202cfa31a31390c7e2e4b838b6778fb133f1b295">elatella/deep-prepyto-paper@202cfa3</a>
on July 15, 2022.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><p><strong>Amin Khosrozadeh</strong><sup><a href="#equal_contribution">*</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" alt="ORCID icon" />
<a href="https://orcid.org/XXXX-XXXX-XXXX-XXXX">XXXX-XXXX-XXXX-XXXX</a>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/ameen-khosrowzadeh">ameen-khosrowzadeh</a><br>
<small>
Institute of Anatomy, University of Bern, Bern, Switzerland; Graduate School for Cellular and Biomedical Sciences, University of Bern
· Funded by Grant XXXXXXXX
</small></p></li>
<li><p><strong>Raphaela Seeger</strong><sup><a href="#equal_contribution">*</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" alt="ORCID icon" />
<a href="https://orcid.org/XXXX-XXXX-XXXX-XXXX">XXXX-XXXX-XXXX-XXXX</a>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/elatella">elatella</a><br>
<small>
Institute of Anatomy, University of Bern, Bern, Switzerland; Graduate School for Cellular and Biomedical Sciences, University of Bern
</small></p></li>
<li><p><strong>Julika Radecke</strong><br>
<img src="images/orcid.svg" class="inline_icon" alt="ORCID icon" />
<a href="https://orcid.org/0000-0002-5815-5537">0000-0002-5815-5537</a>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/julikaradecke">julikaradecke</a><br>
<small>
Institute of Anatomy, University of Bern, Bern, Switzerland; Department of Neuroscience, Faculty of Health and Medical Science , 2200 Copenhagen N, University of Copenhagen, Copenhagen, Denmark; Diamond Light Source Ltd, Didcot, Oxfordshire, United Kingdom
</small></p></li>
<li><p><strong>Guillaume Witz</strong><br>
<img src="images/orcid.svg" class="inline_icon" alt="ORCID icon" />
<a href="https://orcid.org/0000-0003-1562-4265">0000-0003-1562-4265</a>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/guiwitz">guiwitz</a><br>
<small>
Science IT Service, University of Bern, Bern, Switzerland; Microscopy Imaging Center, University of Bern, Bern, Switzerland
</small></p></li>
<li><p><strong>Jakob B. Sørensen</strong><br>
<img src="images/orcid.svg" class="inline_icon" alt="ORCID icon" />
<a href="https://orcid.org/0000-0001-5465-3769">0000-0001-5465-3769</a>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/JBSorensen">JBSorensen</a><br>
<small>
Department of Neuroscience, University of Copenhagen, Blegdamsvej 3B, 2200 Copenhagen N, Denmark
· Funded by Novo Nordisk Fonden, NNF17OC0028516.; Carlsbergfondet, CF17-0875; Independent Research Fond Denmark, 8020-00228A; Lundbeckfonden, R277-2018-802
</small></p></li>
<li><p><strong>Benoît Zuber</strong><sup><a href="#correspondence">✉</a></sup><br>
<img src="images/orcid.svg" class="inline_icon" alt="ORCID icon" />
<a href="https://orcid.org/0000-0001-7725-5579">0000-0001-7725-5579</a>
· <img src="images/github.svg" class="inline_icon" alt="GitHub icon" />
<a href="https://github.com/aseedb">aseedb</a><br>
<small>
Institute of Anatomy, University of Bern, Bern, Switzerland
· Funded by Swiss National Science Foundation, 179520; ERA-NET NEURON, NEURON-119
</small></p></li>
</ul>
<div id="correspondence">
<p>✉ Address correspondence to <a href="mailto:benoit.zuber@ana.unibe.ch" class="email">benoit.zuber@ana.unibe.ch</a>.</p>
</div>
<div id="equal_contribution">
<p>* These authors contributed equally.</p>
</div>
<h2 class="page_break_before" id="abstract">Abstract</h2>
<p>Cryo-electron Tomography (Cryo-ET) has the potential to reveal cell structure down to atomic resolution.
Nevertheless, cellular cryo-ET data is often highly complex and visualization, as well as quantification, of subcellular structures require image segmentation.
Due to a relatively high level of noise and to anisotropic resolution in cryo-ET data, automatic segmentation based on classical computer vision approaches usually does not perform satisfyingly.
For this reason, cryo-ET researchers have mostly performed manual segmentation.</p>
<p>Communication between neurons rely on neurotransmitter-filled synaptic vesicle (SV) exocytosis.
Recruitment of SVs to the plasma membrane is an important means of regulating exocytosis and is influenced by interactions between SVs.
Cryo-ET study of the spatial organization of SVs and of their interconnections allows a better understanding of the mechanisms of exocytosis regulation.
To obtain a faithful representation of SV connectivity state, an absolutely vital prerequisite is an extremely accurate SV segmentation.
Hundreds to thousands of SVs are present in a typical synapse, and their manual segmentation is a burden.
Typically accurately segmenting all SVs in one synapse takes between 3 to 8 days.
This segmentation process has been widely recognized as a bottleneck by the community.</p>
<p>Several attempts to automate vesicle segmentation by classical computer vision or machine learning algorithms have not yielded very robust results.
We addressed this problem by designing a workflow consisting of a U-Net convolutional network followed by post-processing steps.
This combination yields highly accurate results.
Furthermore, we provide an interactive tool for accurately segmenting spherical vesicles in a fraction of the time required by available manual segmentation methods.
This tool can be used to segment vesicles that were missed by the fully automatic procedure or to quickly segment a handful of vesicles, while bypassing the fully automatic procedure.
Our pipeline can in principle be used to segment any spherical vesicle in any cell type as well as extracellular vesicles.</p>
<h2 id="introduction">Introduction</h2>
<p>The fine architecture of cells can be investigated by cryo-electron tomography (cryo-ET) <span class="citation" data-cites="IpfJPPLG">[<a href="#ref-IpfJPPLG" role="doc-biblioref">1</a>]</span>.
Cellular structures are preserved down to the atomic scale through vitrification and observation of the samples in a fully hydrated state.
When a macromolecule is present in a sufficient number of copies in the cells imaged by cryo-ET, it is possible to obtain its atomic structure in situ using subtomogram averaging <span class="citation" data-cites="2TrAHWcN EGfvt7aR">[<a href="#ref-2TrAHWcN" role="doc-biblioref">2</a>,<a href="#ref-EGfvt7aR" role="doc-biblioref">3</a>]</span>.
Cellular cryo-ET datasets are usually extremely complex, making them difficult to analyze.
This is aggravated by the sensitivity of biological samples to electron radiation, which limits the signal-to-noise ratio in cryo-ET datasets <span class="citation" data-cites="WmTa9taa">[<a href="#ref-WmTa9taa" role="doc-biblioref">4</a>]</span>.
Tomographic reconstructions are generated from a series of images of the sample acquired at different viewing angles.
The geometry of the samples prevents acquisition at certain angles, resulting in anisotropic spatial coverage.
The resolution in the directions close to the axis of the electron beam incident on the untilted sample is strongly reduced.
This effect, commonly referred to as the missing-wedge artifact, further complicates data analysis.
In particular, organelles fully bounded by a membrane appear to have holes at their top and bottom (relative to the electron beam axis) <span class="citation" data-cites="WmTa9taa">[<a href="#ref-WmTa9taa" role="doc-biblioref">4</a>]</span>.</p>
<p>The synapse is the functional cellular contact at which information is transmitted from a neuron to another.
The former neuron is called presynaptic and the latter is postsynaptic.
In most cases, the signal is transmitted by the release of neurotransmitters into the intercellular space.
Neurotransmitters are stored in SVs and are released following the fusion of a vesicle with the presynaptic plasma membrane.
A synapse contains hundreds of SVs and their mobility and recruitability for neurotransmitter release depends on inter-vesicle interactions through so-called connector structures <span class="citation" data-cites="XQJ3R1HJ">[<a href="#ref-XQJ3R1HJ" role="doc-biblioref">5</a> ]</span>.
The characterization of these interactions can be performed automatically with the pyto software, which implements a hierarchical connectivity approach to detect and annotate connectors <span class="citation" data-cites="1HtRUUZQi">[<a href="#ref-1HtRUUZQi" role="doc-biblioref">6</a>]</span>. For accurate connector segmentation, an exceptionally precise segmentation of SVs is prerequisite.
To date, this SV segmentation has been achieved manually, but given the massive number of SVs per dataset, it is an extremely time-consuming process.
Typically, one person spends 3 to 8 working days to segment a single dataset.
Attempts to perform this task automatically based on classical computer vision algorithms have not yielded sufficiently accurate performance <span class="citation" data-cites="JWuvfT0Z">[<a href="#ref-JWuvfT0Z" role="doc-biblioref">7</a>]</span>.<br />
To alleviate this situation, we considered applying deep learning methods.</p>
<p>Convolutional neural networks (CNN) have been successfully employed to segment cryo-ET data <span class="citation" data-cites="GNLHO53d">[<a href="#ref-GNLHO53d" role="doc-biblioref">8</a>]</span>.
Although entirely satisfying for visualization purposes, this approach has not met the requirements of pyto.
A recent publication described accurate SV segmentation of transmission electron microscopy images using CNN, but it is limited to 2-dimensional images of resin-embedded synapses <span class="citation" data-cites="RdPTGoZx">[<a href="#ref-RdPTGoZx" role="doc-biblioref">9</a>]</span>.
For our use-case, a common issue is that the input data consists of 2-dimensional images.
In the first study, cryo-ET data are decomposed in individual 2-dimensional slices, which are handed as seperate input to the CNN.
The independent output prediction images are re-assembled in a 3-dimensional stack.<span class="citation" data-cites="GNLHO53d">[<a href="#ref-GNLHO53d" role="doc-biblioref">8</a>]</span>
As discussed above, membranes oriented approximately parallel to the plane of the 2-dimensional tomographic images are not resolved.
In the absence of contextual knowledge of the other 2-dimensional images, the CNN fails to segment these regions of the vesicles.
Hence, spherical vesicles appear open, whereas we expect closed spherical objects.
To overcome this limitation, we used a U-Net CNN that takes 3-dimensional images as input.
U-Net architecture has been introduced in 2015 by Ronenberger et al. <span class="citation" data-cites="ld1EnZ6i">[<a href="#ref-ld1EnZ6i" role="doc-biblioref">10</a>]</span>.
It consists of a contracting path, typical of CNN, and a symmetric expanding path.
At each expansion step, the correspondingly cropped feature map of the contracting path is concatenated.
The contracting path captures context, while the expanding path coupled with concatenation enables precise localization.
The U-Net architecture was developed to achieve a fast and accurate segmentation of biomedical two-dimensional images, with the requirement of only a small fraction of training data in comparison to previous CNNs.
<em>might be redundant</em>
Arranging the sequence of convolution layers in the contraction path along with skip connection and concatenating into the expansion path bring significant privilege for medical and semantic segmentation.
Concurrent with the appearance of 3D convolutional neural networks (CNN) three-dimensional form of U-Net was also developed for volumetric image analysis.</p>
<p>It was then extended to segment 3-dimensional biomedical images (3D U-Net) <span class="citation" data-cites="D7hXMn0y">[<a href="#ref-D7hXMn0y" role="doc-biblioref">11</a>]</span>.
Weigert et al. <span class="citation" data-cites="12G712Zky">[<a href="#ref-12G712Zky" role="doc-biblioref">12</a>]</span> implemented a U-Net for content-aware restoration (CARE) of 3-dimensional fluorescence microscopy datasets. They showed that it can restore information from anisotropic and very noisy datasets.</p>
<p>We implemented a 3D U-Net based on CARE building blocks and trained it with manually segmented datasets.
This method provided good accuracy and was only slightly affected by the missing wedge artifact.
Nevertheless, it was not quite sufficient for our downstream pyto analysis.
Hence, we developed a post-processing method, which transforms the segmented objects into spheres and refines their radius and center location.
This leads to a substantial accuracy improvement, which are reflected in better pyto performance.
Additionally, we designed a multivariate ranking procedure, highlighting possibly wrongly segmented SVs.
We also introduce a semi-automatic method to very quickly fix wrongly segmented and missed SVs.</p>
<p>Although our set of procedures was developed with the use case of SV segmentation in mind, it can be used to segment any other types of biological spherical vesicles, such as transport vesicles, secretory vesicles, endocytic vesicles, and extracellular vesicles.</p>
<h2 id="results">Results</h2>
<p>To automize the tracing of cellular features in tomograms, also called segmentation, we developed a deep-learning pipeline, which segments 3D objects.
-&gt; similar approaches in 2D
The manual segmentation of synaptic vesicles is one of the most time-intensive steps, when segmenting tomograms of presynaptic terminals.
Synaptic vesicles also constitute a large, homogeneous group, which would create a large training set for deep learning applications.
Therefore, we decided to develop the automatic segmentation for synaptic vesicles in an initial step.
The used tomograms were previously manually segmented with IMOD <span class="citation" data-cites="136NHHp17">[<a href="#ref-136NHHp17" role="doc-biblioref">13</a>]</span>.
In a next step, filaments connecting the synaptic vesicles with each other (connectors) and to the active zone (AZ) were automatically segmented with the algorithm application Pyto <span class="citation" data-cites="1HtRUUZQi">[<a href="#ref-1HtRUUZQi" role="doc-biblioref">6</a>]</span>.</p>
<div id="fig:pipeline" class="fignos">
<figure>
<img src="images/pipeline.svg" style="width:15cm" alt="Figure 1: Pipeline of automatic segmentation. a) tomograms b) patchify the tomograms into 3D patches c) Segmentation Network d) probability masks e) stitching patches back f) thresholding g) adaptive localized thresholding h) outlier removal i) radial profile" /><figcaption aria-hidden="true"><span>Figure 1:</span> <strong>Pipeline of automatic segmentation.</strong> a) tomograms b) patchify the tomograms into 3D patches c) Segmentation Network d) probability masks e) stitching patches back f) thresholding g) adaptive localized thresholding h) outlier removal i) radial profile</figcaption>
</figure>
</div>
<div id="fig:tom" class="fignos">
<figure>
<img src="images/tomo-sclae.svg" style="width:15cm" alt="Figure 2: 2D Slices A) a section from z axis of a tomogram’s presynaptic terminal of a neuron B) instance mask of the vesicles after post processing C) predicted probability mask by the segmentation network" /><figcaption aria-hidden="true"><span>Figure 2:</span> <strong>2D Slices</strong> A) a section from z axis of a tomogram’s presynaptic terminal of a neuron B) instance mask of the vesicles after post processing C) predicted probability mask by the segmentation network</figcaption>
</figure>
</div>
<div id="fig:tom" class="fignos">
<figure>
<img src="images/improvment-neurons.png" style="width:15cm" alt="Figure 3: Dice Imporvment Dice improvements after post processing of initial predicted mask (different colors correspond to different tomograms ): a) training datasets b) synaptosome test datasets c) Neuron test datasets" /><figcaption aria-hidden="true"><span>Figure 3:</span> <strong>Dice Imporvment</strong> Dice improvements after post processing of initial predicted mask (different colors correspond to different tomograms ): a) training datasets b) synaptosome test datasets c) Neuron test datasets</figcaption>
</figure>
</div>
<div id="fig:radial_profile" class="fignos">
<figure>
<img src="images/radial_avg_115-099.svg" style="width:15cm" alt="Figure 4: Vesicle radius and position through radial profile and cross-correlation Radial Profile Refinement A) couple of vesicles are not centered B) Radial Profile. Blue range is from membrane center to outer white halo center, this is the search range for the optimal radius. (smoothed by gaussian filtering) C) second derivative of radial profile E, F, H, G) Same as above columns after refinement" /><figcaption aria-hidden="true"><span>Figure 4:</span> <strong>Vesicle radius and position through radial profile and cross-correlation</strong> Radial Profile Refinement A) couple of vesicles are not centered B) Radial Profile. Blue range is from membrane center to outer white halo center, this is the search range for the optimal radius. (smoothed by gaussian filtering) C) second derivative of radial profile E, F, H, G) Same as above columns after refinement</figcaption>
</figure>
</div>
<p>Fig 6- Splitting adjacent vesicles. A) Examples of tomogram, no labels; B) raw label with connected vesicle-labels; C) modified label with seperated vesicles —&gt; for software: IMOD</p>
<h3 id="comparison-of-manual-segmentation-with-automatic-deep-learning-based-segmentation">Comparison of manual segmentation with automatic deep-learning based segmentation</h3>
<div id="fig:3d" class="fignos">
<figure>
<img src="images/3d.png" style="width:15cm" alt="Figure 5: 3D model of manual segmented and automatically segmented synaptosome." /><figcaption aria-hidden="true"><span>Figure 5:</span> <strong>3D model of manual segmented and automatically segmented synaptosome.</strong></figcaption>
</figure>
</div>
<p>Evaluation metric DICE for pixel/pixel analysis</p>
<p>Table 1- Evaluation of the segmentation- MDICE: Mask Dice coefficient for the predicted mask PDICE: Dice coefficient after post-processing SIGMA-d: diameter error on correctly detected vesicle, DELTA-c: average error center (nm) #Vesicles: number of expected vesicles TP: True Positive FN: False Negative FP: False Positive</p>
<h1 id="final-eval-tables">Final Eval Tables</h1>
<h2 id="train-dataset">Train Dataset</h2>
<table>
<thead>
<tr class="header">
<th><strong><em>Dataset</em></strong></th>
<th style="text-align: center;"><strong><em>Mask DICE</em></strong></th>
<th style="text-align: center;"><strong><em>Final Label DICE</em></strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong><em>δ d</em></strong></th>
<th style="text-align: center;"><strong><em>Δ c (nm)</em></strong></th>
<th style="text-align: center;"><strong><em># of Vesicles</em></strong></th>
<th style="text-align: center;"><strong>TP</strong></th>
<th style="text-align: center;"><strong>FN</strong></th>
<th style="text-align: center;"><strong>FP</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Synaptosome C1</td>
<td style="text-align: center;">0.44</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">2.55±1.56</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">198</td>
<td style="text-align: center;">26</td>
<td style="text-align: center;">49</td>
</tr>
<tr class="even">
<td>Synaptosome C2</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">2.12±1.06</td>
<td style="text-align: center;">105</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td>Synaptosome C3</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.86±1.24</td>
<td style="text-align: center;">128</td>
<td style="text-align: center;">127</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td>Synaptosome C4</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">1.78±0.92</td>
<td style="text-align: center;">144</td>
<td style="text-align: center;">141</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">4</td>
</tr>
<tr class="odd">
<td>Synaptosome C5</td>
<td style="text-align: center;">0.58</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.86±1.00</td>
<td style="text-align: center;">214</td>
<td style="text-align: center;">209</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">13</td>
</tr>
<tr class="even">
<td>Synaptosome C6</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.84</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">1.92±1.05</td>
<td style="text-align: center;">104</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">16</td>
</tr>
<tr class="odd">
<td>Synaptosome C7</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">1.86±0.90</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">184</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">16</td>
</tr>
<tr class="even">
<td>Synaptosome C8</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.9</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.70±0.93</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td>Synaptosome C9</td>
<td style="text-align: center;">0.59</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.87±0.91</td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">132</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">14</td>
</tr>
<tr class="even">
<td><strong>Average</strong></td>
<td style="text-align: center;"><strong>0.64±0.11</strong></td>
<td style="text-align: center;"><strong>0.86±0.05</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>0.05</strong></td>
<td style="text-align: center;"><strong>1.95±1.08</strong></td>
<td style="text-align: center;"><strong>152.22</strong></td>
<td style="text-align: center;"><strong>97.00%</strong></td>
<td style="text-align: center;"><strong>3.00%</strong></td>
<td style="text-align: center;"><strong>7.30%</strong></td>
</tr>
</tbody>
</table>
<h2 id="test-dataset-same-preparation-and-microscope-with-training-set">Test Dataset (Same preparation and microscope with training set)</h2>
<table>
<thead>
<tr class="header">
<th><strong><em>Dataset</em></strong></th>
<th style="text-align: center;"><strong><em>Mask DICE</em></strong></th>
<th style="text-align: center;"><strong><em>Final Label DICE</em></strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong><em>δ d</em></strong></th>
<th style="text-align: center;"><strong><em>Δ c (nm)</em></strong></th>
<th style="text-align: center;"><strong><em># of Vesicles</em></strong></th>
<th style="text-align: center;"><strong>TP</strong></th>
<th style="text-align: center;"><strong>FN</strong></th>
<th style="text-align: center;"><strong>FP</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Synaptosome C10</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.88</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">1.86±1.18</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">5</td>
</tr>
<tr class="even">
<td>Synaptosome T1</td>
<td style="text-align: center;">0.75</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">2.66±1.52</td>
<td style="text-align: center;">699</td>
<td style="text-align: center;">687</td>
<td style="text-align: center;">12</td>
<td style="text-align: center;">33</td>
</tr>
<tr class="odd">
<td>Synaptosome T2</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">2.27±1.84</td>
<td style="text-align: center;">122</td>
<td style="text-align: center;">117</td>
<td style="text-align: center;">5</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="even">
<td>Synaptosome T3</td>
<td style="text-align: center;">0.72</td>
<td style="text-align: center;">0.74</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.11</td>
<td style="text-align: center;">3.64±2.22</td>
<td style="text-align: center;">434</td>
<td style="text-align: center;">397</td>
<td style="text-align: center;">37</td>
<td style="text-align: center;">57</td>
</tr>
<tr class="odd">
<td>Synaptosome T5</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">2.20±1.26</td>
<td style="text-align: center;">535</td>
<td style="text-align: center;">526</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">25</td>
</tr>
<tr class="even">
<td>Synaptosome T6</td>
<td style="text-align: center;">0.6</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">2.02±1.12</td>
<td style="text-align: center;">373</td>
<td style="text-align: center;">353</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">42</td>
</tr>
<tr class="odd">
<td>Synaptosome T7</td>
<td style="text-align: center;">0.8</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">2.22±1.14</td>
<td style="text-align: center;">110</td>
<td style="text-align: center;">107</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="even">
<td>Synaptosome T8</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;">0.91</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">2.09±1.04</td>
<td style="text-align: center;">100</td>
<td style="text-align: center;">99</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td>Synaptosome T10</td>
<td style="text-align: center;">0.77</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.96±1.04</td>
<td style="text-align: center;">77</td>
<td style="text-align: center;">74</td>
<td style="text-align: center;">3</td>
<td style="text-align: center;">6</td>
</tr>
<tr class="even">
<td><strong>Average</strong></td>
<td style="text-align: center;"><strong>0.75±0.06</strong></td>
<td style="text-align: center;"><strong>0.83±0.05</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>0.08</strong></td>
<td style="text-align: center;"><strong>2.32±1.43</strong></td>
<td style="text-align: center;"><strong>286.56</strong></td>
<td style="text-align: center;"><strong>96.30%</strong></td>
<td style="text-align: center;"><strong>3.70%</strong></td>
<td style="text-align: center;"><strong>6.10%</strong></td>
</tr>
</tbody>
</table>
<h2 id="test-dataset-3-neuron-dataset">Test Dataset 3 (Neuron Dataset)</h2>
<table>
<thead>
<tr class="header">
<th><strong><em>Dataset</em></strong></th>
<th style="text-align: center;"><strong><em>Mask DICE</em></strong></th>
<th style="text-align: center;"><strong><em>Final Label DICE</em></strong></th>
<th style="text-align: center;"></th>
<th style="text-align: center;"><strong><em>δ d</em></strong></th>
<th style="text-align: center;"><strong><em>Δ c (nm)</em></strong></th>
<th style="text-align: center;"><strong><em># of Vesicles</em></strong></th>
<th style="text-align: center;"><strong>TP</strong></th>
<th style="text-align: center;"><strong>FN</strong></th>
<th style="text-align: center;"><strong>FP</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Neuron 133</td>
<td style="text-align: center;">0.76</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">2.16±1.32</td>
<td style="text-align: center;">523</td>
<td style="text-align: center;">467</td>
<td style="text-align: center;">56</td>
<td style="text-align: center;">8</td>
</tr>
<tr class="even">
<td>Neuron 123</td>
<td style="text-align: center;">0.64</td>
<td style="text-align: center;">0.71</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">2.05±1.18</td>
<td style="text-align: center;">66</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">8</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td>Neuron 84</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;">0.89</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">1.44±0.75</td>
<td style="text-align: center;">498</td>
<td style="text-align: center;">484</td>
<td style="text-align: center;">14</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="even">
<td>Neuron 134</td>
<td style="text-align: center;">0.56</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.09</td>
<td style="text-align: center;">2.87±2.50</td>
<td style="text-align: center;">638</td>
<td style="text-align: center;">384</td>
<td style="text-align: center;">254</td>
<td style="text-align: center;">63</td>
</tr>
<tr class="odd">
<td>Neuron 115</td>
<td style="text-align: center;">0.57</td>
<td style="text-align: center;">0.63</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.08</td>
<td style="text-align: center;">3.56±3.23</td>
<td style="text-align: center;">170</td>
<td style="text-align: center;">123</td>
<td style="text-align: center;">47</td>
<td style="text-align: center;">32</td>
</tr>
<tr class="even">
<td>Neuron 102</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;">0.86</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.05</td>
<td style="text-align: center;">1.47±0.79</td>
<td style="text-align: center;">103</td>
<td style="text-align: center;">86</td>
<td style="text-align: center;">17</td>
<td style="text-align: center;">1</td>
</tr>
<tr class="odd">
<td>Neuron 80</td>
<td style="text-align: center;">0.7</td>
<td style="text-align: center;">0.81</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">2.67±2.00</td>
<td style="text-align: center;">111</td>
<td style="text-align: center;">102</td>
<td style="text-align: center;">9</td>
<td style="text-align: center;">3</td>
</tr>
<tr class="even">
<td>Neuron 114</td>
<td style="text-align: center;">0.65</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">2.68±1.79</td>
<td style="text-align: center;">131</td>
<td style="text-align: center;">93</td>
<td style="text-align: center;">38</td>
<td style="text-align: center;">9</td>
</tr>
<tr class="odd">
<td>Neuron 132</td>
<td style="text-align: center;">0.69</td>
<td style="text-align: center;">0.87</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.03</td>
<td style="text-align: center;">1.65±1.26</td>
<td style="text-align: center;">135</td>
<td style="text-align: center;">129</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">32</td>
</tr>
<tr class="even">
<td>Neuron 73</td>
<td style="text-align: center;">0.78</td>
<td style="text-align: center;">0.83</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.06</td>
<td style="text-align: center;">2.93±2.00</td>
<td style="text-align: center;">526</td>
<td style="text-align: center;">483</td>
<td style="text-align: center;">43</td>
<td style="text-align: center;">2</td>
</tr>
<tr class="odd">
<td>Neuron 128</td>
<td style="text-align: center;">0.67</td>
<td style="text-align: center;">0.85</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.04</td>
<td style="text-align: center;">2.33±1.70</td>
<td style="text-align: center;">252</td>
<td style="text-align: center;">232</td>
<td style="text-align: center;">20</td>
<td style="text-align: center;">19</td>
</tr>
<tr class="even">
<td>Neuron 116</td>
<td style="text-align: center;">0.62</td>
<td style="text-align: center;">0.73</td>
<td style="text-align: center;"></td>
<td style="text-align: center;">0.07</td>
<td style="text-align: center;">2.38±1.82</td>
<td style="text-align: center;">296</td>
<td style="text-align: center;">207</td>
<td style="text-align: center;">89</td>
<td style="text-align: center;">35</td>
</tr>
<tr class="odd">
<td><strong>Average</strong></td>
<td style="text-align: center;"><strong>0.69±0.09</strong></td>
<td style="text-align: center;"><strong>0.79±0.09</strong></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"><strong>0.06</strong></td>
<td style="text-align: center;"><strong>2.35±1.83</strong></td>
<td style="text-align: center;"><strong>287.42</strong></td>
<td style="text-align: center;"><strong>83.60%</strong></td>
<td style="text-align: center;"><strong>16.40%</strong></td>
<td style="text-align: center;"><strong>7.90%</strong></td>
</tr>
</tbody>
</table>
<p>Global analysis</p>
<div id="fig:dice" class="fignos">
<figure>
<img src="images/blinddice.png" style="width:15cm" alt="Figure 6: Dice coefficient and loss value for training and validation set." /><figcaption aria-hidden="true"><span>Figure 6:</span> <strong>Dice coefficient and loss value for training and validation set.</strong></figcaption>
</figure>
</div>
<p>3d unet good for 3D processing
recent Nature methods paper by Ben Engel, DeepFinder -&gt; Relion for STA creates mask to find more using dl
-what are they doing, maybe compare that in the text, different aims; we might compare results we achieve (keep as bonus, revision)</p>
<p>3 different types of dl: classification, localization and segmentation
We specialize in accuratly segmenting 3D svs -&gt; results
through the network, output is a mask, smooth DICE/ binary DICE = which one for our mask? one function for both? -&gt; Amin will check
when we encounter error until now we remove sv(?), bug inside function -&gt; fix bug for better performance, how many fail and why?
evaluation: objectwise evaluation for I/O, radius, center (according to nature paper) -&gt; compare with manual seg
other common evaluation tool other than DICE (Amin wants to check)</p>
<h2 id="discussion">Discussion</h2>
<p>DICE</p>
<h3 id="outlook">Outlook</h3>
<p>implement automatic cell-outline and active zone segmentation as deep learning workflow using UNet
implement automatic connector and tether segmentation as a deep leaning workflow using UNet</p>
<h2 id="materials-and-methods">Materials and methods</h2>
<h3 id="cryo-electron-tomography-datasets">Cryo-electron Tomography Datasets</h3>
<p>Two datasets of different origin were used as input and test subjects for the automatic segmentation pipeline, rat synaptosomes as well as astrocytic and neural cell cultures derived from mice.
Both datasets have been previously analyzed <span class="citation" data-cites="CERJ8H0p">[<a href="#ref-CERJ8H0p" role="doc-biblioref">14</a>]</span>.</p>
<h4 id="rat-synaptosomes">Rat synaptosomes</h4>
<p>The preparation of the rat synaptosomes were previously described <span class="citation" data-cites="7sGZgtQa">[<a href="#ref-7sGZgtQa" role="doc-biblioref">15</a>]</span>.
After the purification, the synaptosomes were incubated for 30min at room temperature.
1.3mM CaCl2 and 10 nm gold fiducials were added (gold fiducials, #s10110/8. AURION Immuno Gold Reagents &amp; Accessories. Wageningen, The Neatherlands).
The synaptosome solution was applied to a 200-mesh lacey finder carbon film grid (#AGS166-H2. Agar Scientific. Elektron Technology UK Ltd. Stansted, UK).
Manual blotting was used to remove exess liquid with the aid of a filter paper.
Thereafter the grid was immediately plunge frozen in liquid ethane using a homebuilt plunge freezer controlled with a LabView script (National Onstruments Corporation. Mopac Expwy Austin, TX, USA).
The grids coated with rat synaptosomes were mounted in a cryo-holder (Gatan, Pleasonton, CA, USA) and transferred to a Tecnai F20 (FEI, Eindhoven, The Netherlands) which was set to low dose conditions, operated at 200 kV, and equipped with a field emission gun.
Images were recorded with a 2k x 2k CCD camera (Gatan) mounted after a GIF Tridiem post-column filter (Gatan) operated in zero-loss mode.
The sample was kept at about -180°C.
Tilt series were acquired using SerialEM <span class="citation" data-cites="19ZFerhph">[<a href="#ref-19ZFerhph" role="doc-biblioref">16</a>]</span> for automated acquisition recorded typically from -50° to 50° with a 2° angular increment and an unbinned pixel size of 0.75 or 1.2 nm.
Due to sample thickness (400-700 nm), tomograms were usually not recorded with higher tilt angles.
Defocus was set between -8 to -12 µm and the total electron dose used was about 80-100 e<sup>-</sup>/Å<sup>2</sup>.
Some tomograms were acquired at a Titan Krios equipped with a K2 direct electron detector (Gatan) without energy filter.
The K2 camera was operated in superresolution counting mode and between 8-40 frames per tilt angle were taken.
Tilt series were acquired using the Latitude software (Gatan) for automated acquisition recorded typically from -60° to 60° with a 2° angular increment and an unbinned pixel size of 0.6 nm.
Defocus was set between -8 to -12 µm and the total electron dose used was about 80-100 e<sup>-</sup>/Å<sup>2</sup>.
Prior to image processing the frames at each tilt angle, frames were aligned and averaged in 2dx MC_Automator <span class="citation" data-cites="7UkCfakv">[<a href="#ref-7UkCfakv" role="doc-biblioref">17</a>]</span> with motioncor <span class="citation" data-cites="4rVTIFyw">[<a href="#ref-4rVTIFyw" role="doc-biblioref">18</a>]</span>.
3D reconstruction was done in IMOD <span class="citation" data-cites="136NHHp17">[<a href="#ref-136NHHp17" role="doc-biblioref">13</a>]</span>.
The alignments were done using the automated fiducial tracking function and the 3D reconstructions were done using the weighted back projection followed by a nonlinear anisotropic diffusion (NAD) filtering.</p>
<h4 id="astrocytic-and-neuronal-mouse-culture">Astrocytic and neuronal mouse culture</h4>
<p>The preparation of astrocytic and neuronal culture has been published before <span class="citation" data-cites="5DZLPDB7">[<a href="#ref-5DZLPDB7" role="doc-biblioref">19</a>]</span>.
After 12 to 14 days of incubation grids with mouse neurons were plunge frozen with a Vitrobot (Thermofisher Scientific, Mark IV) with a blot time of 3 s and a blot force of -10.
Wait time and drain time were not used.
Humidity was set to 100% at 4°C.
4 µl undiluteted 10 nm BSA gold tracer (Aurion) was added directly onto the grid prior to plunge freezing.
Cultured mouse neurons tilt series were acquired at a Titan Krios, equipped with a Falcon 3 direct electron detector (Thermofisher Scientific) without energy filter.
The Falcon camera was operated in linear mode.
Tilt series were acquired using the TEM Tomography software (TFS) for automated acquisition recorded typically from -60° to 60° with a 2° angular increment and an unbinned pixel size of 0.37 nm.
Defocus was set between -6 to -10 µm and the total electron dose used was about 80-100 e<sup>-</sup>/Å<sup>2</sup>.
Tomogram reconstruction was done in the same way as for the synaptosome datasets.</p>
<h3 id="manual-segmentation-procedures">Manual segmentation procedures</h3>
<p>Manual segmentation of SVs, mitochondria, and the active zone PM was done in IMOD (Figure S4A&amp;B).
The boundary marked the region to be analyzed by Pyto <span class="citation" data-cites="1HtRUUZQi">[<a href="#ref-1HtRUUZQi" role="doc-biblioref">6</a>]</span>.
The analysis by Pyto was essentially the same as described previously <span class="citation" data-cites="XQJ3R1HJ 1HtRUUZQi">[<a href="#ref-XQJ3R1HJ" role="doc-biblioref">5</a>,<a href="#ref-1HtRUUZQi" role="doc-biblioref">6</a>]</span>.</p>
<p>In short, the segmented area is divided in 1 voxel thick layers parallel to the active zone for distance calculations.
A hierarchical connectivity segmentation detects densities interconnecting vesicles (so called connectors) and densities connecting vesicles to the active zone PM (so called tethers) (Figure <code class="green">\_add figure number*</code>).
Distance calculations are done with the center of the vesicle.
Mainly default settings were used.
The segmentation procedure is conservative and tends to miss some tethers and connectors because of noise.
Consequently, the numbers of tethers and connectors should not be considered as absolute values but rather to compare experimental groups.
As it was done before, an upper limit was set between 2100 and 3200 nm<sup>3</sup> on segment volume.
The tomograms that were used for this analysis were binned by a factor of 2 to 3, resulting in voxel sizes between 2.1 and 2.4 nm.</p>
<h3 id="pre-processing-of-manual-segmentation-outputs-from-imod-for-further-use-jupyter-notebook-pre-pyto">Pre-processing of manual segmentation outputs from IMOD for further use (jupyter notebook pre-pyto)</h3>
<h3 id="description-of-machine-learning">Description of Machine Learning</h3>
<p>This experiment was conducted on a workstation with an AMD Ryzen Threadripper 3970X CPU with a NVIDIA GeForce RTX 2080 Ti GPU.
All the framework has been implemented in Python using the Keras library (2.4.3) and Tensorflow (2.4.1).
The workflow includes a GUI based on a multi-dimensional image viewer, Napari (0.4.15), enabling the user to add and remove vesicles.
<code class="green">\_any other packages required?*</code></p>
<p>The used datasets included a total of 30 tomograms with heterogeneous pixel sizes, defocus and resolution.</p>
<h4 id="deep-model-training-with-a-3d-u-net">Deep Model Training with a 3D U-Net</h4>
<h5 id="create-an-input-voxel-patch">Create an Input Voxel Patch</h5>
<p>The training set was prepared by splitting the 3D tomographic volume into 32x32x32 <code class="green">\_voxels??*</code> sub volumes and keeping only volumes occupied by a sufficient amount (&gt; 1000 voxels) of binarized vesicle labels.
The obtained sub volumes were randomly divided into ten subsets of the training set, this method is termed k-fold cross-validation in the field of machine learning.
All of these subsets or “folds” were used as training sets, as an entirely seperate set of tomographic subvolumes was used for validation, to avoid overfitting.</p>
<p><code class="green">\_are the folds overlapping? are the tomograms normalized any further than the NAD from previous segementation before feeding it to the deeplearning model? add image from odt??, the 2D slices of the subset are supposed to be 32x32x32, but seem to have a different size... padding?? ---&gt; results*</code></p>
<h5 id="d-u-net-architecture">3D U-Net architecture</h5>
<p>The previously prepared subsets are fed into the 3D U-Net in batches of 50. <code class="green">\_cite Ronneberger, what was changed compared to the original U-Net, is there a better publication to cite?*</code>
These were passed throught the U-Net in a total of 200 epochs.
Batch normalization was applied before Rectified Lienear Unit (ReLU) activation <code class="green">\_was it? cite Ioffe &amp; Szegedy, maybe? [@doi:10.48550/arXiv.1502.03167]*</code>.</p>
<p>The 3D U-Net architecture is composed of a contracting or analysis path (convolutional layers), and expanding or synthesis path (deconvolutional layers) (Figure <a href="#fig:unet">7</a>).
<code class="green">\_did we wrote our own U-Net or do we need to quote someones github for the original framework*</code></p>
<div id="fig:unet" class="fignos">
<figure>
<img src="images/unet.png" style="width:10cm" alt="Figure 7: Segmentation Network: U-Net. Input Size is 32^3 \_voxels??*, in each resolution we have two convolution layer followed by batch normalization layer and rectified Linear Units (ReLU) activation function. Intermediate sizes are written on top of arrows, number of convolution filters is written bottom of boxes. Skip connections shows concatenation of the features from contracting path (left side of the network) and expansive path (right side of the network).\_explain different colors, do they correspond with the different steps? for example: red- max pooling 2x2x2? add figure legend?*" /><figcaption aria-hidden="true"><span>Figure 7:</span> <strong>Segmentation Network: U-Net.</strong> Input Size is 32^3 <code>\_voxels??*</code>, in each resolution we have two convolution layer followed by batch normalization layer and rectified Linear Units (ReLU) activation function. Intermediate sizes are written on top of arrows, number of convolution filters is written bottom of boxes. Skip connections shows concatenation of the features from contracting path (left side of the network) and expansive path (right side of the network).<code class="green">\_explain different colors, do they correspond with the different steps? for example: red- max pooling 2x2x2? add figure legend?*</code></figcaption>
</figure>
</div>
<p><code class="green">\_what do we use as initial encoder weights and backbone*</code>
During each layer of the analysis path, the convolution filter consiting of a 3x3x3 kernel, with randomly initialized weights, was moved over all the voxels in each subset twice, each time taking their dot product.
This kernel extracts and enhances the features in different parts of the image.
This is followed by a ReLU function, which can be described as</p>
<p><span class="math display">\[f(x)=max(0,x)\]</span></p>
<p>which will output the positive input directly, while setting negative input to 0.</p>
<p>Between each layer, the voxels were condensed, or downsampled, via a max-pooling step of 2x2x2, in which the maximum value of the 2x2x2 voxel cube is put forth.
With every layer of the U-Net, the input size thereby halfs, while the number of channels doubles.</p>
<p>While the analysis path focusses on the identification of what to segement, the synthesis part focusses on their localization.
The synthesis path of the U-Net uses the same basic architecture as the analysis path, with a slight variation of using up convolution operation and implementing skip connections, where feature maps of the analysis part are concatunated to the output of the transposed convolutions of the same level.</p>
<p>The sigmoid activation function is the last block of the U-Net (Figure <a href="#fig:unet">7</a>), it triggers the loss function, which
As the segementation of vesicles can be considered a binary classification task, binary cross-entropy was implemented as a loss function.</p>
<p><span class="math display">\[Loss= -\frac{1}{output  size}\sum_{i=1}^{output size} y_i * log  ŷ_i + (1-y_1) * log  (1-ŷ_1)\]</span></p>
<p>with the <em>output size</em> being the nuber of scalar values in the model output, <em>ŷ_1_</em> being the <em>i</em>-th scalar value in the model output and <em>y_i_</em> being the corresponding target value.
<code class="green">\_publication to cite?*</code></p>
<p>The binary cross-entropy lossfunction converts the output from the decoder path into a mask, where each voxel is assigned as either vesicle or not-vesicle.</p>
<p>training: weights -&gt; Adam optimizer for the training of the network <span class="citation" data-cites="1G9auqG3f">[<a href="#ref-1G9auqG3f" role="doc-biblioref">20</a>]</span>.</p>
<p>“The back-propagation is done to update the weights and reduce the loss function with the help of an optimizer - the most common optimizer algorithm is gradient descent. Multiple epochs are run until the loss converges to the global minima.”</p>
<p>Come after the convolutional layers to achieve a 3D probability mask a Softmax layer applies to bring channel size to one.</p>
<h5 id="mask-prediction">Mask prediction</h5>
<p>We split large tomograms into 32x32x32 patches with step size of 24 (stride) and then stitch together the predictions to get the final probability mask.</p>
<h4 id="transfer-learning">Transfer Learning</h4>
<h3 id="postprocessing">Postprocessing</h3>
<h4 id="estimating-global-threshold">Estimating Global Threshold</h4>
<p>Whereas in the tomograms, vesicle’s membrance are darker from inside and background, in order to binarize the obtained probability mask, we search through some thresholds (from 0.8 to 1.0) and select the one that minimizes average of luminance of all marginal voxels.(srounnd of vesicles)
<code class="green">\_add image from odt??*</code></p>
<h4 id="adaptive-localized-threshold">Adaptive Localized Threshold</h4>
<p>although the global threshold reveals almost all desired synaptic vesicles due to variation in the intensity of vesicles surrounding some binarized labels that are far away from the spherical shape of vesicles. First, by looking at the extent value(Ratio of pixels in the region to pixels in the total bounding box. Computed as area / (rows * cols)) and the size of each particle’s binary label we can capture those vesicles that are get connected and those vesicles that captured partially.
Then by searching more into the probability mask, we try to expand the partially detected vesicles and separate those close connected vesicles by searching between the initial threshold and one to find a better finner threshold that can separate these vesicles.</p>
<h4 id="radial-profile">Radial Profile</h4>
<h4 id="outlier-removal">Outlier Removal</h4>
<p>We define feature space on predicted vesicles’ label containing thickness, membrane density, and estimated radius of a vesicle. (Benoit: after radial profile, we can add the definition of thinness and membrane as well)
While we face different metrics for detecting outliers we apply Mahalanobis Distance MD on this multivariate space MD to calculate L2 norm distance on normalized variable using the covariance matrix of observation. Afterward, we calculate the p-value of MD and bring this evaluation setup in iterative form while giving the second chance to detect outlier vesicles while recalculating radial profile in a specific margin range (0-10) and removing them if they could not pass a margin on the p-value.</p>
<h4 id="radius-estimation-cross-correlation-through-radial-profile">Radius Estimation (Cross Correlation through Radial Profile)</h4>
<h3 id="analysis-of-results">Analysis of Results</h3>
<p>We design the evaluation framework to show robust capabilities of the proposed toolbox on synaptic vesicles segmentation which not only to be content with quantitative evaluation on the neural network performance but rather assay the segmentation of vesicles in practice and using the output of the segmentation with another pre-developed toolbox for segmenting tethers and connectors in presynaptic.</p>
<ol type="1">
<li>Dataset: All tomograms that we partially segmented and used for training (Synaptosome)</li>
<li>A single tomogram with the exactly same setup and sample preparation like the train dataset</li>
<li>Dataset: 8 Synaptosome tomogram with ground truth (with an exceeding treatment on the samples)</li>
<li>Dataset: 12 Neuron fully segmented tomograms with completely different sample preparation and microscope setup</li>
</ol>
<h4 id="dice">DICE</h4>
<p>The quantification of the performance of the model while training is calculated with the general form of dice coefficient for the probabilistic maps and after stitching the probabilistic mask of patches back together and building the tomogram probabilistic map we have another calculation on the whole tomogram for evaluating the similarity of the predicted probability mask with ground truth. The binarization from the same formulation converges to this interpretation that we measure the overlap between two samples.</p>
<p><span class="math display">\[1-\frac{2\sum_{pixels} y_{true} y_{pred}%}{\sum_{pixels} y_{true}^2+\sum_{pixels} y_{pred}^2}\]</span></p>
<p><code class="green">\_shouln't it be voxels instead of pixels??*</code></p>
<p>We monitored all the stages of post-processing on the eventual label file with DICE to see the effect of each post-processing step and we report the final label’s DICE.</p>
<p>However dice coefficient is a good global measure to assess our prediction in comparison to ground truth but it is far away from how individually vesicles are segmented. For example, a single generated vesicle label containing several close connected vesicles would not be practical for further analysis for the researcher although it could have almost the same dice value. What is important for actual usage of the software would be the number and percentage of true-detected vesicles, false-positive and false-negative rates. We can also calculate the error of the estimated diameter and center. We define a vesicle as a true-detected vesicle if the predicted center is in the hand-segmented vesicle and the other way around the center of prediction is in the predicted vesicle. This means the volume of intersection of the estimated vesicle with the distance of d to a ground truth vesicle with radius R is:</p>
<p><span class="math display">\[V=\frac{1}{12}\pi(4R+d)(2R-d)\]</span></p>
<h4 id="diameter-error">Diameter Error</h4>
<p>The relevant characterization of each vesicle would diameter of vesicles which also we used to remove outliers as well (radius of vesicles in that case). The error of diameter estimation of true-detected vesicle is defined as 1 minus the proportion of diameters</p>
<p><span id="eq:regular-equation" class="eqnos"><span class="math display">\[\delta d=1-\frac{min(dSi,dGTi)}{max(dSi,dGTi)}\]</span><span class="eqnos-number">(1)</span></span> </p>
<p>where dGTi diameter of each true manual segmented vesicle, and dSi is the diameter of its estimation.</p>
<h4 id="center-error">Center error</h4>
<p>The center error is a euclidean distance of ground truth and corresponding true predicted vesicles. Besides this calculation, if we measure each axis error it will reveal that human bias in segmentation is more affected on the Z-axis. [we didn’t show it in number but its checked the hypothesis]</p>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-IpfJPPLG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>Towards Visual Proteomics at High Resolution</strong> <div class="csl-block">Felix JB Bäuerlein, Wolfgang Baumeister</div> <em>Journal of Molecular Biology</em> (2021-10) <a href="https://doi.org/gn9t3v">https://doi.org/gn9t3v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jmb.2021.167187">10.1016/j.jmb.2021.167187</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34384780">34384780</a></div></div>
</div>
<div id="ref-2TrAHWcN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>Exploring high-resolution cryo-ET and subtomogram averaging capabilities of contemporary DEDs</strong> <div class="csl-block">Martin Obr, Wim JH Hagen, Robert A Dick, Lingbo Yu, Abhay Kotecha, Florian KM Schur</div> <em>Molecular Biology</em> (2022-01-10) <a href="https://doi.org/gn92pd">https://doi.org/gn92pd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2022.01.10.475481">10.1101/2022.01.10.475481</a></div></div>
</div>
<div id="ref-EGfvt7aR" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>High-resolution in situ structure determination by cryo-electron tomography and subtomogram averaging using emClarity</strong> <div class="csl-block">Tao Ni, Thomas Frosio, Luiza Mendonça, Yuewen Sheng, Daniel Clare, Benjamin A Himes, Peijun Zhang</div> <em>Nature Protocols</em> (2022-02) <a href="https://doi.org/gn92pc">https://doi.org/gn92pc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41596-021-00648-5">10.1038/s41596-021-00648-5</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/35022621">35022621</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9251519">PMC9251519</a></div></div>
</div>
<div id="ref-WmTa9taa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>STRUCTURAL STUDIES BY ELECTRON TOMOGRAPHY: From Cells to Molecules</strong> <div class="csl-block">Vladan Lučić, Friedrich Förster, Wolfgang Baumeister</div> <em>Annual Review of Biochemistry</em> (2005-06-01) <a href="https://doi.org/cfd27f">https://doi.org/cfd27f</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1146/annurev.biochem.73.011303.074112">10.1146/annurev.biochem.73.011303.074112</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/15952904">15952904</a></div></div>
</div>
<div id="ref-XQJ3R1HJ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Quantitative analysis of the native presynaptic cytomatrix by cryoelectron tomography</strong> <div class="csl-block">Rubén Fernández-Busnadiego, Benoît Zuber, Ulrike Elisabeth Maurer, Marek Cyrklaff, Wolfgang Baumeister, Vladan Lučić</div> <em>Journal of Cell Biology</em> (2010-01-11) <a href="https://doi.org/b9c26b">https://doi.org/b9c26b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1083/jcb.200908082">10.1083/jcb.200908082</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/20065095">20065095</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2812849">PMC2812849</a></div></div>
</div>
<div id="ref-1HtRUUZQi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>Hierarchical detection and analysis of macromolecular complexes in cryo-electron tomograms using Pyto software</strong> <div class="csl-block">Vladan Lučić, Rubén Fernández-Busnadiego, Ulrike Laugks, Wolfgang Baumeister</div> <em>Journal of Structural Biology</em> (2016-12) <a href="https://doi.org/f9d5t2">https://doi.org/f9d5t2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jsb.2016.10.004">10.1016/j.jsb.2016.10.004</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27742578">27742578</a></div></div>
</div>
<div id="ref-JWuvfT0Z" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>Robust membrane detection based on tensor voting for electron tomography</strong> <div class="csl-block">Antonio Martinez-Sanchez, Inmaculada Garcia, Shoh Asano, Vladan Lucic, Jose-Jesus Fernandez</div> <em>Journal of Structural Biology</em> (2014-04) <a href="https://doi.org/f5zkj8">https://doi.org/f5zkj8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jsb.2014.02.015">10.1016/j.jsb.2014.02.015</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24625523">24625523</a></div></div>
</div>
<div id="ref-GNLHO53d" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>Convolutional neural networks for automated annotation of cellular cryo-electron tomograms</strong> <div class="csl-block">Muyuan Chen, Wei Dai, Stella Y Sun, Darius Jonasch, Cynthia Y He, Michael F Schmid, Wah Chiu, Steven J Ludtke</div> <em>Nature Methods</em> (2017-10) <a href="https://doi.org/gkpj62">https://doi.org/gkpj62</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nmeth.4405">10.1038/nmeth.4405</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28846087">28846087</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5623144">PMC5623144</a></div></div>
</div>
<div id="ref-RdPTGoZx" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>Automated Detection and Localization of Synaptic Vesicles in Electron Microscopy Images</strong> <div class="csl-block">Barbara Imbrosci, Dietmar Schmitz, Marta Orlando</div> <em>eneuro</em> (2022-01) <a href="https://doi.org/gn9n8k">https://doi.org/gn9n8k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1523/eneuro.0400-20.2021">10.1523/eneuro.0400-20.2021</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34983830">34983830</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8805189">PMC8805189</a></div></div>
</div>
<div id="ref-ld1EnZ6i" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>U-Net: Convolutional Networks for Biomedical Image Segmentation</strong> <div class="csl-block">Olaf Ronneberger, Philipp Fischer, Thomas Brox</div> <em>arXiv</em> (2015-05-19) <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></div>
</div>
<div id="ref-D7hXMn0y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><strong>3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation</strong> <div class="csl-block">Özgün Çiçek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, Olaf Ronneberger</div> <em>arXiv</em> (2016-06-22) <a href="https://arxiv.org/abs/1606.06650">https://arxiv.org/abs/1606.06650</a></div>
</div>
<div id="ref-12G712Zky" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Content-aware image restoration: pushing the limits of fluorescence microscopy</strong> <div class="csl-block">Martin Weigert, Uwe Schmidt, Tobias Boothe, Andreas Müller, Alexandr Dibrov, Akanksha Jain, Benjamin Wilhelm, Deborah Schmidt, Coleman Broaddus, Siân Culley, … Eugene W Myers</div> <em>Nature Methods</em> (2018-12) <a href="https://doi.org/gfkkfd">https://doi.org/gfkkfd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-018-0216-7">10.1038/s41592-018-0216-7</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30478326">30478326</a></div></div>
</div>
<div id="ref-136NHHp17" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>Computer Visualization of Three-Dimensional Image Data Using IMOD</strong> <div class="csl-block">James R Kremer, David N Mastronarde, JRichard McIntosh</div> <em>Journal of Structural Biology</em> (1996-01) <a href="https://doi.org/d9nfzw">https://doi.org/d9nfzw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1006/jsbi.1996.0013">10.1006/jsbi.1996.0013</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/8742726">8742726</a></div></div>
</div>
<div id="ref-CERJ8H0p" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>Morphofunctional changes at the active zone during synaptic vesicle exocytosis</strong> <div class="csl-block">Julika Radecke, Raphaela Seeger, Anna Kádková, Ulrike Laugks, Amin Khosrozadeh, Kenneth N Goldie, Vladan Lučić, Jakob B Sørensen, Benoît Zuber</div> <em>Neuroscience</em> (2022-03-07) <a href="https://doi.org/gpm26v">https://doi.org/gpm26v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/2022.03.07.483217">10.1101/2022.03.07.483217</a></div></div>
</div>
<div id="ref-7sGZgtQa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>A rapid Percoll gradient procedure for preparation of synaptosomes</strong> <div class="csl-block">Peter R Dunkley, Paula E Jarvie, Phillip J Robinson</div> <em>Nature Protocols</em> (2008-11) <a href="https://doi.org/b7zwh8">https://doi.org/b7zwh8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nprot.2008.171">10.1038/nprot.2008.171</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18927557">18927557</a></div></div>
</div>
<div id="ref-19ZFerhph" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Automated electron microscope tomography using robust prediction of specimen movements</strong> <div class="csl-block">David N Mastronarde</div> <em>Journal of Structural Biology</em> (2005-10) <a href="https://doi.org/ff7gzx">https://doi.org/ff7gzx</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jsb.2005.07.007">10.1016/j.jsb.2005.07.007</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/16182563">16182563</a></div></div>
</div>
<div id="ref-7UkCfakv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>2dx_automator: Implementation of a semiautomatic high-throughput high-resolution cryo-electron crystallography pipeline</strong> <div class="csl-block">Sebastian Scherer, Julia Kowal, Mohamed Chami, Venkata Dandey, Marcel Arheit, Philippe Ringler, Henning Stahlberg</div> <em>Journal of Structural Biology</em> (2014-05) <a href="https://doi.org/f522h3">https://doi.org/f522h3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jsb.2014.03.016">10.1016/j.jsb.2014.03.016</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24680783">24680783</a></div></div>
</div>
<div id="ref-4rVTIFyw" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>Electron counting and beam-induced motion correction enable near-atomic-resolution single-particle cryo-EM</strong> <div class="csl-block">Xueming Li, Paul Mooney, Shawn Zheng, Christopher R Booth, Michael B Braunfeld, Sander Gubbens, David A Agard, Yifan Cheng</div> <em>Nature Methods</em> (2013-06) <a href="https://doi.org/f4zpjf">https://doi.org/f4zpjf</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nmeth.2472">10.1038/nmeth.2472</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23644547">23644547</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3684049">PMC3684049</a></div></div>
</div>
<div id="ref-5DZLPDB7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline"><strong>Preparation of Primary Neurons for Visualizing Neurites in a Frozen-hydrated State Using Cryo-Electron Tomography</strong> <div class="csl-block">Sarah H Shahmoradian, Mauricio R Galiano, Chengbiao Wu, Shurui Chen, Matthew N Rasband, William C Mobley, Wah Chiu</div> <em>Journal of Visualized Experiments</em> (2014-02-12) <a href="https://doi.org/gmh9w3">https://doi.org/gmh9w3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3791/50783">10.3791/50783</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24561719">24561719</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4089403">PMC4089403</a></div></div>
</div>
<div id="ref-1G9auqG3f" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline"><strong>Adam: A Method for Stochastic Optimization</strong> <div class="csl-block">Diederik P Kingma, Jimmy Ba</div> <em>arXiv</em> (2014) <a href="https://doi.org/hnkr">https://doi.org/hnkr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1412.6980">10.48550/arxiv.1412.6980</a></div></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen())
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script>
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
